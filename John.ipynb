{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35896b8a-cd5a-48d9-86d0-e26155074501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d8ad06-864c-4f6c-a722-33538003a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734a101d-fd2b-4bde-be96-57059d505ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/07 15:54:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/07 15:54:16 WARN Utils: Service 'sparkDriver' could not bind on port 9999. Attempting port 10000.\n",
      "24/03/07 15:54:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/03/07 15:54:17 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 10005. Attempting port 10006.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.130:7077\") \\\n",
    "        .appName(\"John\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"300s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://192.168.2.130:9000\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58643b28-c3e4-45b1-bb27-d66209ffe420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/07 15:54:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/03/07 15:54:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/03/07 15:55:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/03/07 15:55:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/03/07 15:55:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/03/07 15:55:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "24/03/07 15:56:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .h5 files: ['hdfs://192.168.2.130:9000/user/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "direct_path = \"hdfs://192.168.2.130:9000/user/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5\"\n",
    "\n",
    "df = spark_session.read.format(\"binaryFile\").load(direct_path)\n",
    "h5_file_paths = df.select(\"path\").rdd.map(lambda row: row.path).collect()\n",
    "print(\"Found .h5 files:\", h5_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9a9ac15-d916-4fce-bd73-def30b9818f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File hdfs://192.168.2.130:9000/user/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# store = pd.HDFStore(direct_path, 'w')\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# pdf = pd.read_hdf(direct_path)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dhf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirect_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/pytables.py:418\u001b[0m, in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m     exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists:\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_buf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    420\u001b[0m store \u001b[38;5;241m=\u001b[39m HDFStore(path_or_buf, mode\u001b[38;5;241m=\u001b[39mmode, errors\u001b[38;5;241m=\u001b[39merrors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# can't auto open/close if we are using an iterator\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# so delegate to the iterator\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File hdfs://192.168.2.130:9000/user/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5 does not exist"
     ]
    }
   ],
   "source": [
    "# store = pd.HDFStore(direct_path, 'w')\n",
    "# pdf = pd.read_hdf(direct_path)\n",
    "\n",
    "dhf = pd.read_hdf(direct_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d17c618a-1b4e-4a53-806b-1a6a8d7358cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method HDFStore.keys of <class 'pandas.io.pytables.HDFStore'>\n",
      "File path: direct_path\n",
      ">\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'No object named file path in the file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/pytables.py:841\u001b[0m, in \u001b[0;36mHDFStore.select\u001b[0;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close)\u001b[0m\n\u001b[1;32m    839\u001b[0m group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_node(key)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo object named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# create the storer and axes\u001b[39;00m\n\u001b[1;32m    844\u001b[0m where \u001b[38;5;241m=\u001b[39m _ensure_term(where, scope_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'No object named file path in the file'"
     ]
    }
   ],
   "source": [
    "print(store.keys)\n",
    "store.select('direct_path') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c23b73-45ff-46d9-ab7a-c4132c17fd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 .h5 files.\n"
     ]
    }
   ],
   "source": [
    "sc = spark_session.sparkContext\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "file_system = sc._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n",
    "\n",
    "def list_h5_files(path, fs=file_system):\n",
    "    files = []\n",
    "    status = fs.listStatus(sc._jvm.org.apache.hadoop.fs.Path(path))\n",
    "    for file_status in status:\n",
    "        file_path = file_status.getPath()\n",
    "        if file_status.isDirectory():\n",
    "            files.extend(list_h5_files(file_path.toString()))\n",
    "        elif file_path.toString().endswith(\".h5\"):\n",
    "            files.append(file_path.toString())\n",
    "    return files\n",
    "\n",
    "base_directory = \"hdfs://192.168.2.130:9000/user/MillionSongSubset\"\n",
    "h5_files = list_h5_files(base_directory)\n",
    "print(f\"Found {len(h5_files)} .h5 files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f38ef0-d984-49d8-801f-66a3a0d22e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hdfs://192.168.2.130:9000/user/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5', 'hdfs://192.168.2.130:9000/user/MillionSongSubset/A/A/A/TRAAABD128F429CF47.h5', 'hdfs://192.168.2.130:9000/user/MillionSongSubset/A/A/A/TRAAADZ128F9348C2E.h5']\n"
     ]
    }
   ],
   "source": [
    "print(h5_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6db1755-9725-43f8-9b41-58464a6d7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5_to_row(filename):\n",
    "    \"\"\"\n",
    "    Read a single HDF5 file and return a tuple representing a row of data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(filename, 'r') as file:\n",
    "            return (\n",
    "                float(file['analysis']['songs'][0][23]),\n",
    "                int(file['musicbrainz']['songs'][0][1]),\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7ebf76e-353f-4bc6-9f66-1aa46c6d0755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file hdfs://192.168.2.130:9000/user/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5: [Errno 2] Unable to synchronously open file (unable to open file: name = 'hdfs://192.168.2.130:9000/user/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "#file_paths_rdd = sc.parallelize(h5_files[:10])\n",
    "# song_data_rdd = spark_session.sparkContext.parallelize(h5_files[:10]).map(read_h5_to_row)\n",
    "read_h5_to_row(direct_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e99340-4166-4167-b394-d8e361155bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_data_from_h5_in_memory(file_content):\n",
    "#     data = {}\n",
    "#     try:\n",
    "#         with h5py.File(io.BytesIO(file_content), 'r') as h5_file:\n",
    "#             data['loudness'] = h5_file['/analysis/songs'][0][23]\n",
    "#             data['year'] = h5_file['/analysis/songs'][0][1]\n",
    "#     except Exception as e:\n",
    "#         data = {'error': str(e)}\n",
    "#     return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cef7bba5-faa8-4753-8f60-dd97d8ab415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow\n",
    "\n",
    "# def read_h5_file_from_hdfs(hdfs_path, hdfs_host='default', hdfs_port=9000):\n",
    "#     # Connect to HDFS\n",
    "#     hdfs = pyarrow.HadoopFileSystem(host=hdfs_host, port=hdfs_port)\n",
    "    \n",
    "#     # Read the file into memory\n",
    "#     with hdfs.open(hdfs_path, 'rb') as f:\n",
    "#         file_content = f.read()\n",
    "    \n",
    "#     return file_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc400678-26c3-4064-ba0d-49af40818b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = read_h5_file_from_hdfs(direct_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36f8e1c3-b6f9-4f26-8c53-63b534164d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_file(file_path):\n",
    "#     file_content = read_h5_file_from_hdfs(file_path)\n",
    "#     return extract_data_from_h5_in_memory(file_content)\n",
    "\n",
    "# # Distribute the work\n",
    "# file_paths_rdd = sc.parallelize(h5_files[:10])\n",
    "# extracted_data_rdd = file_paths_rdd.map(lambda file_path: process_file(file_path))\n",
    "\n",
    "# # # Collect the results\n",
    "# # extracted_data = extracted_data_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8fda029-07d4-4090-8262-ebb6775cab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in extracted_data_rdd.take(5):\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a3f9072-6935-4cfe-909b-f24044d23787",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
