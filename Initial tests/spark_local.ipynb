{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = 4\n",
    "partitions = 4\n",
    "spark_session = SparkSession.builder.appName(\"John\").master(f\"local[{cores}]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 4\n",
      "Partitions: 4\n",
      "Partitions: 4\n"
     ]
    }
   ],
   "source": [
    "def read_h5_to_row(filename):\n",
    "    \"\"\"\n",
    "    Read a single HDF5 file and return a tuple representing a row of data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(filename, 'r') as file:\n",
    "            return (\n",
    "\n",
    "                float(file['analysis']['songs'][0][23]),\n",
    "\n",
    "                int(file['musicbrainz']['songs'][0][1]),\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "song_paths = glob.glob('./MillionSongSubset/*/*/*/*.h5')\n",
    "\n",
    "# Use Spark to parallelize the file processing and read the data\n",
    "song_data_rdd = spark_session.sparkContext.parallelize(song_paths[:8000]).map(read_h5_to_row)\n",
    "\n",
    "print(f'Partitions: {song_data_rdd.getNumPartitions()}')\n",
    "song_data_rdd = song_data_rdd.repartition(partitions)\n",
    "print(f'Partitions: {song_data_rdd.getNumPartitions()}')\n",
    "\n",
    "# Convert RDD to Spark DataFrame\n",
    "schema = StructType([\n",
    "\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "\n",
    "    StructField(\"year\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "columns = ['loudness', 'year']\n",
    "df = spark_session.createDataFrame(song_data_rdd, schema=schema)\n",
    "print(f'Partitions: {df.rdd.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# song_data_rdd = song_data_rdd.map(lambda x: tuple(float(y) if isinstance(y, np.float64) else y for y in x))\n",
    "\n",
    "# for item in song_data_rdd.take(5):\n",
    "#     print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(song_data_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark_session.createDataFrame(song_data_rdd, schema=schema)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 4\n"
     ]
    }
   ],
   "source": [
    "# df = spark_session.read.csv(\"test.csv\", header=True, inferSchema=True)#.cache()\n",
    "# df.show(5)\n",
    "filtered_df = df.filter(col(\"year\") != 0)\n",
    "# filtered_df = filtered_df.repartition(10)\n",
    "print(f'Partitions: {filtered_df.rdd.getNumPartitions()}')\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assembler = VectorAssembler(inputCols=[\"year\"], outputCol=\"features\")\n",
    "assembled_df = assembler.transform(filtered_df)\n",
    "# assembled_df.show(10, truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.11658413690774709, -242.5665448140215]\n",
      "execution time: 17.690048217773438\n",
      "Partitions: 4\n",
      "number of cores: 4\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"loudness\")\n",
    "model = lr.fit(assembled_df)\n",
    "coefficients = [model.coefficients[0], model.intercept]\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(f'execution time: {duration}')\n",
    "print(f'Partitions: {assembled_df.rdd.getNumPartitions()}')\n",
    "print(f'number of cores: {cores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Assuming df is your original DataFrame with the feature and target columns\n",
    "# # Assuming lr_model is your trained linear regression model\n",
    "\n",
    "# # Extract feature and target data from DataFrame\n",
    "# feature_data = filtered_df.select(\"year\").collect()  # Replace \"feature_column\" with your actual feature column name\n",
    "# target_data = filtered_df.select(\"loudness\").collect()    # Replace \"target_column\" with your actual target column name\n",
    "\n",
    "# # Convert to NumPy arrays\n",
    "# feature_data = np.array([row[0] for row in feature_data])\n",
    "# target_data = np.array([row[0] for row in target_data])\n",
    "\n",
    "# # Plot scatter plot of feature vs. target\n",
    "# plt.scatter(feature_data, target_data, 5, 'k')\n",
    "\n",
    "# # Generate the line corresponding to linear regression\n",
    "# x_values = np.linspace(min(feature_data), max(feature_data), 100)\n",
    "# y_values = coefficients[0] * x_values + coefficients[1]\n",
    "\n",
    "# # Plot the linear regression line\n",
    "# plt.plot(x_values, y_values, color='red', label=\"Linear Regression\")\n",
    "\n",
    "# # Add labels and legend\n",
    "# plt.xlabel(\"Year\")\n",
    "# plt.ylabel(\"Loudness\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Show plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
